{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "glove.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XZrmkj1epQXP",
        "colab": {}
      },
      "source": [
        "#Абдыкалыкова Асель"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfEr4J-CDjFD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import torch.functional as F\n",
        "import torch.nn.functional as F\n",
        "from scipy import sparse"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PXzybySkpW58",
        "outputId": "4f0cc238-258e-4301-9da3-1b93a14d6134",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "text_1 = ['It is Python code to implement glove but it is not finished yet because code is just only until the cooccurence part']\n",
        "\n",
        "#Function dividing text into words\n",
        "def tokenize(text):\n",
        "    tokens = [x.lower().split() for x in text]\n",
        "    return tokens\n",
        "token_size=len(tokens)\n",
        "tokens = tokenize(text_1)\n",
        "\n",
        "#Creating vocabulary with unique words with index\n",
        "vocabulary = []\n",
        "for word in tokens:\n",
        "    for token in word:\n",
        "        if token not in vocabulary:\n",
        "            vocabulary.append(token)\n",
        "word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
        "idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
        "vocabulary_size = len(vocabulary)\n",
        "\n",
        "print(tokens)\n",
        "print(word2idx)\n",
        "print(idx2word)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['it', 'is', 'python', 'code', 'to', 'implement', 'glove', 'but', 'it', 'is', 'not', 'finished', 'yet', 'because', 'code', 'is', 'just', 'only', 'until', 'the', 'cooccurence', 'part']]\n",
            "{'it': 0, 'is': 1, 'python': 2, 'code': 3, 'to': 4, 'implement': 5, 'glove': 6, 'but': 7, 'not': 8, 'finished': 9, 'yet': 10, 'because': 11, 'just': 12, 'only': 13, 'until': 14, 'the': 15, 'cooccurence': 16, 'part': 17}\n",
            "{0: 'it', 1: 'is', 2: 'python', 3: 'code', 4: 'to', 5: 'implement', 6: 'glove', 7: 'but', 8: 'not', 9: 'finished', 10: 'yet', 11: 'because', 12: 'just', 13: 'only', 14: 'until', 15: 'the', 16: 'cooccurence', 17: 'part'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Dlw-vCc6p-ma",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "df09947c-ff91-4571-c10a-a1f2634e2625"
      },
      "source": [
        "cooccurence=np.zeros((vocabulary_size,vocabulary_size))\n",
        "token = tokens[0]\n",
        "for idx in range (len(token)-1):\n",
        "    ind1 = word2idx[token[idx]]\n",
        "    ind2 = word2idx[token[idx+1]]\n",
        "    \n",
        "    \n",
        "    cooccurence[ind1,ind2]+=1\n",
        "print(cooccurence)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}